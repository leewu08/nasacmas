{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5666c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e08b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# build_shap.py\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m BASE    \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18;43m__file__\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m DATA    \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m MODEL   \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# build_shap.py\n",
    "import os, joblib, pandas as pd, shap\n",
    "\n",
    "BASE    = os.path.dirname(__file__)\n",
    "DATA    = os.path.join(BASE, 'data')\n",
    "MODEL   = os.path.join(DATA, 'model.pkl')\n",
    "VAL_CSV = os.path.join(DATA, 'val_results.csv')\n",
    "OUT     = os.path.join(DATA, 'shap_values.pkl')\n",
    "\n",
    "# 1) 필수 파일 체크\n",
    "for p in (MODEL, VAL_CSV):\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"파일 없음: {p}\")\n",
    "\n",
    "# 2) 데이터 로드 & feature만 골라 ndarray 생성\n",
    "df    = pd.read_csv(VAL_CSV)\n",
    "drops = ['unit','RUL','y_true','y_pred']\n",
    "feats = [c for c in df.columns if c not in drops]\n",
    "X     = df[feats].values    # (n_samples, n_features)\n",
    "\n",
    "# 3) 모델 로드\n",
    "model = joblib.load(MODEL)\n",
    "\n",
    "# 4) KernelExplainer 생성\n",
    "#    - 모델 예측 함수: model.predict (ndarray → ndarray)\n",
    "#    - background: X 중 랜덤 100개 (속도 조절 가능)\n",
    "bg_idx    = list(range(min(100, X.shape[0])))\n",
    "background = X[bg_idx]\n",
    "explainer  = shap.KernelExplainer(model.predict, background)\n",
    "\n",
    "# 5) SHAP 값 계산\n",
    "n = X.shape[0]\n",
    "print(f\"SHAP 계산 시작: {n} samples, {X.shape[1]} features\")\n",
    "# 속도 위해 nsamples=100 (필요시 늘리세요)\n",
    "shap_out = explainer.shap_values(X, nsamples=100)\n",
    "# shap_out이 리스트면 첫 번째 엘리먼트 사용\n",
    "shap_vals = shap_out[0] if isinstance(shap_out, (list,tuple)) else shap_out\n",
    "\n",
    "# 6) unit별 dict 정리\n",
    "units     = df['unit'].tolist()\n",
    "shap_data = {\n",
    "    u: {'feature_names': feats, 'values': shap_vals[i].tolist()}\n",
    "    for i, u in enumerate(units)\n",
    "}\n",
    "\n",
    "# 7) 저장\n",
    "joblib.dump(shap_data, OUT)\n",
    "print(f\"✔ shap_values.pkl 생성 완료: {OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceba634d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ test_cleaned feature_cols: ['time', 'op1', 'op2', 'op3', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
      "Provided model function fails when applied to the provided data set.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32, 25), dtype=float32). Expected shape (None, 50, 24), but input has incompatible shape (32, 25)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 25), dtype=float32)\n  • training=False\n  • mask=None\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(MODEL)\n\u001b[0;32m     29\u001b[0m background \u001b[38;5;241m=\u001b[39m X[: \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m100\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m---> 30\u001b[0m explainer  \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKernelExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# 5) SHAP 계산\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m▶ SHAP 계산 시작: samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, features=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\olzld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\shap\\explainers\\_kernel.py:97\u001b[0m, in \u001b[0;36mKernelExplainer.__init__\u001b[1;34m(self, model, data, feature_names, link, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m convert_to_model(model, keep_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m convert_to_data(data, keep_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index)\n\u001b[1;32m---> 97\u001b[0m model_null \u001b[38;5;241m=\u001b[39m \u001b[43mmatch_model_to_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# enforce our current input type limitations\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, (DenseData, SparseData)):\n",
      "File \u001b[1;32mc:\\Users\\olzld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\shap\\utils\\_legacy.py:142\u001b[0m, in \u001b[0;36mmatch_model_to_data\u001b[1;34m(model, data)\u001b[0m\n\u001b[0;32m    140\u001b[0m         out_val \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mf(data\u001b[38;5;241m.\u001b[39mconvert_to_df())\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m         out_val \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided model function fails when applied to the provided data set.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\olzld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\olzld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\models\\functional.py:276\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    274\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    275\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m     )\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32, 25), dtype=float32). Expected shape (None, 50, 24), but input has incompatible shape (32, 25)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 25), dtype=float32)\n  • training=False\n  • mask=None\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "# build_shap.py (수정)\n",
    "import os, joblib, pandas as pd, shap\n",
    "\n",
    "BASE    = os.getcwd()\n",
    "DATA    = os.path.join(BASE, 'data')\n",
    "MODEL   = os.path.join(DATA, 'model.pkl')\n",
    "# val_results.csv 대신 FD001 test cleaned 사용\n",
    "TEST_CSV= os.path.join(DATA, 'test_FD001_cleaned.csv')\n",
    "OUT     = os.path.join(DATA, 'shap_values.pkl')\n",
    "\n",
    "# 1) 파일 체크\n",
    "for p in (MODEL, TEST_CSV):\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"파일 없음: {p}\")\n",
    "\n",
    "# 2) test CSV 로드 & 마지막 타임스텝만 추출\n",
    "df_test = pd.read_csv(TEST_CSV)\n",
    "latest  = df_test.groupby('unit').last().reset_index()\n",
    "\n",
    "# 3) feature_cols 자동 감지 (unit, RUL 제외)\n",
    "drop_cols = ['unit','RUL']\n",
    "feats     = [c for c in latest.columns if c not in drop_cols]\n",
    "print(\"▶ test_cleaned feature_cols:\", feats)\n",
    "\n",
    "X = latest[feats].values  # ndarray (n_units, n_features)\n",
    "\n",
    "# 4) 모델 로드 & KernelExplainer 생성\n",
    "model = joblib.load(MODEL)\n",
    "background = X[: min(100, X.shape[0])]\n",
    "explainer  = shap.KernelExplainer(model.predict, background)\n",
    "\n",
    "# 5) SHAP 계산\n",
    "print(f\"▶ SHAP 계산 시작: samples={X.shape[0]}, features={X.shape[1]}\")\n",
    "shap_out = explainer.shap_values(X, nsamples=100)\n",
    "shap_vals = shap_out[0] if isinstance(shap_out, (list,tuple)) else shap_out\n",
    "\n",
    "# 6) dict 정리 & 저장\n",
    "units = latest['unit'].tolist()\n",
    "shap_data = {\n",
    "    u: {'feature_names': feats, 'values': shap_vals[i].tolist()}\n",
    "    for i, u in enumerate(units)\n",
    "}\n",
    "joblib.dump(shap_data, OUT)\n",
    "print(\"✔ shap_values.pkl 생성 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b7b0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\temp\\\\nasa_cmas\\\\cmaps\\\\data\\\\data\\\\test_FD001_cleaned.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 1) 파일 로드\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model   \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(MODEL)\n\u001b[1;32m---> 13\u001b[0m df_test \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEST_CSV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 2) 마지막 시점만 추출\u001b[39;00m\n\u001b[0;32m     16\u001b[0m latest \u001b[38;5;241m=\u001b[39m df_test\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munit\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mlast()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[1;32mc:\\Users\\olzld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\olzld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\olzld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\olzld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\olzld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\temp\\\\nasa_cmas\\\\cmaps\\\\data\\\\data\\\\test_FD001_cleaned.csv'"
     ]
    }
   ],
   "source": [
    "# build_shap.py\n",
    "import os, joblib, pandas as pd, shap\n",
    "\n",
    "# 1) 경로 설정\n",
    "BASE     = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE, 'data')\n",
    "MODEL_PKL= os.path.join(DATA_DIR, 'model.pkl')\n",
    "TEST_CSV = os.path.join(DATA_DIR, 'test_FD001_cleaned.csv')  # FD001 예시\n",
    "OUT_PKL  = os.path.join(DATA_DIR, 'shap_values.pkl')\n",
    "\n",
    "# 2) 필수 파일 확인\n",
    "for p in (MODEL_PKL, TEST_CSV):\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"파일이 없습니다: {p}\")\n",
    "\n",
    "# 3) 테스트 데이터 로드 & 마지막 타임스텝만\n",
    "df_test = pd.read_csv(TEST_CSV)\n",
    "latest  = df_test.groupby('unit').last().reset_index()\n",
    "\n",
    "# 4) feature 컬럼 자동 감지\n",
    "drop_cols = ['unit','RUL']\n",
    "feats     = [c for c in latest.columns if c not in drop_cols]\n",
    "print(\"▶ 감지된 features:\", feats)\n",
    "\n",
    "X = latest[feats].values   # numpy array (n_units, n_features)\n",
    "\n",
    "# 5) 모델 로드\n",
    "model = joblib.load(MODEL_PKL)\n",
    "\n",
    "# 6) KernelExplainer 준비\n",
    "#    - model.predict의 결과를 1D array로 flatten\n",
    "predict_fn = lambda x: model.predict(x).flatten()\n",
    "bg_size    = min(100, X.shape[0])\n",
    "background = X[:bg_size]\n",
    "explainer  = shap.KernelExplainer(predict_fn, background)\n",
    "\n",
    "# 7) SHAP 값 계산\n",
    "print(f\"▶ SHAP 계산: samples={X.shape[0]}, features={X.shape[1]}\")\n",
    "shap_out = explainer.shap_values(X, nsamples=100)\n",
    "# list 반환이면 첫 번째 요소 사용\n",
    "shap_vals = shap_out[0] if isinstance(shap_out, (list, tuple)) else shap_out\n",
    "\n",
    "# 8) shap_data dict 생성 & 저장\n",
    "units     = latest['unit'].tolist()\n",
    "shap_data = {\n",
    "    u: {'feature_names': feats, 'values': shap_vals[i].tolist()}\n",
    "    for i, u in enumerate(units)\n",
    "}\n",
    "joblib.dump(shap_data, OUT_PKL)\n",
    "print(f\"✔ shap_values.pkl 생성 완료: {OUT_PKL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd99ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ feature_cols: ['time', 'op1', 'op2', 'op3', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olzld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:94: UserWarning: Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n",
      "  warnings.warn(\n",
      "c:\\Users\\olzld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: input_layer_3\n",
      "Received: inputs=['Tensor(shape=(100, 25))']\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input [[ 3.10000e+01 -6.00000e-04  4.00000e-04 ...  1.00000e+02  3.88100e+01\n   2.33552e+01]\n [ 4.90000e+01  1.80000e-03 -1.00000e-04 ...  1.00000e+02  3.88100e+01\n   2.32618e+01]\n [ 1.26000e+02 -1.60000e-03  4.00000e-04 ...  1.00000e+02  3.89300e+01\n   2.32740e+01]\n ...\n [ 1.21000e+02  1.70000e-03  1.00000e-04 ...  1.00000e+02  3.87600e+01\n   2.33608e+01]\n [ 9.70000e+01  4.70000e-03 -0.00000e+00 ...  1.00000e+02  3.89500e+01\n   2.33595e+01]\n [ 1.98000e+02  1.30000e-03  3.00000e-04 ...  1.00000e+02  3.87000e+01\n   2.31855e+01]]. Expected shape (None, 50, 24), but input has incompatible shape (100, 25)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=['tf.Tensor(shape=(100, 25), dtype=float32)']\n  • training=None\n  • mask=['None']\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m bg_size    \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m100\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     38\u001b[0m background \u001b[38;5;241m=\u001b[39m X[:bg_size]\n\u001b[1;32m---> 39\u001b[0m explainer  \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeepExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# 7) SHAP 값 계산\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m▶ Deep SHAP 계산: samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, features=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\olzld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\shap\\explainers\\_deep\\__init__.py:93\u001b[0m, in \u001b[0;36mDeepExplainer.__init__\u001b[1;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeep_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TFDeep\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer \u001b[38;5;241m=\u001b[39m \u001b[43mTFDeep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_phase_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeep_pytorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyTorchDeep\n",
      "File \u001b[1;32mc:\\Users\\olzld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:161\u001b[0m, in \u001b[0;36mTFDeep.__init__\u001b[1;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# if type(self.model)is tuple:\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;66;03m#    self.fModel(cnn.inputs, cnn.get_layer(theNameYouWant).outputs)\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_value \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_between_tensors(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_output\u001b[38;5;241m.\u001b[39mop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_inputs)\n",
      "File \u001b[1;32mc:\\Users\\olzld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\olzld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\models\\functional.py:276\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    274\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    275\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m     )\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input [[ 3.10000e+01 -6.00000e-04  4.00000e-04 ...  1.00000e+02  3.88100e+01\n   2.33552e+01]\n [ 4.90000e+01  1.80000e-03 -1.00000e-04 ...  1.00000e+02  3.88100e+01\n   2.32618e+01]\n [ 1.26000e+02 -1.60000e-03  4.00000e-04 ...  1.00000e+02  3.89300e+01\n   2.32740e+01]\n ...\n [ 1.21000e+02  1.70000e-03  1.00000e-04 ...  1.00000e+02  3.87600e+01\n   2.33608e+01]\n [ 9.70000e+01  4.70000e-03 -0.00000e+00 ...  1.00000e+02  3.89500e+01\n   2.33595e+01]\n [ 1.98000e+02  1.30000e-03  3.00000e-04 ...  1.00000e+02  3.87000e+01\n   2.31855e+01]]. Expected shape (None, 50, 24), but input has incompatible shape (100, 25)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=['tf.Tensor(shape=(100, 25), dtype=float32)']\n  • training=None\n  • mask=['None']\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "# build_shap.py\n",
    "import os, joblib, pandas as pd, shap\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1) 경로 설정\n",
    "BASE      = os.getcwd()\n",
    "DATA_DIR  = os.path.join(BASE, 'data')\n",
    "MODEL_PKL = os.path.join(DATA_DIR, 'model.pkl')\n",
    "TEST_CSV  = os.path.join(DATA_DIR, 'test_FD001_cleaned.csv')\n",
    "OUT_PKL   = os.path.join(DATA_DIR, 'shap_values.pkl')\n",
    "\n",
    "# 2) 필수 파일 확인\n",
    "for p in (MODEL_PKL, TEST_CSV):\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"파일이 없습니다: {p}\")\n",
    "\n",
    "# 3) 테스트 데이터 로드 & 마지막 타임스텝만\n",
    "df_test = pd.read_csv(TEST_CSV)\n",
    "latest  = df_test.groupby('unit').last().reset_index()\n",
    "\n",
    "# 4) feature 컬럼 자동 감지 & ndarray 변환\n",
    "drop_cols = ['unit','RUL']\n",
    "feats     = [c for c in latest.columns if c not in drop_cols]\n",
    "print(\"▶ feature_cols:\", feats)\n",
    "X = latest[feats].values.astype(float)  # ensure float32/64\n",
    "\n",
    "# 5) 모델 로드\n",
    "#    만약 joblib로 Keras 모델을 저장/불러왔으면, \n",
    "#    대신 아래처럼 load_model 쓰세요:\n",
    "# from tensorflow.keras.models import load_model\n",
    "# model = load_model(MODEL_KERAS_H5)\n",
    "model = joblib.load(MODEL_PKL)  \n",
    "\n",
    "# 6) DeepExplainer 생성\n",
    "#    DeepExplainer는 내부적으로 gradient 기반 SHAP을 쓰며,\n",
    "#    TF/Keras 모델에 최적화되어 있습니다.\n",
    "bg_size    = min(100, X.shape[0])\n",
    "background = X[:bg_size]\n",
    "explainer  = shap.DeepExplainer(model, background)\n",
    "\n",
    "# 7) SHAP 값 계산\n",
    "print(f\"▶ Deep SHAP 계산: samples={X.shape[0]}, features={X.shape[1]}\")\n",
    "shap_out   = explainer.shap_values(X)  \n",
    "# shap_out도 list일 수 있으니:\n",
    "shap_vals  = shap_out[0] if isinstance(shap_out, (list, tuple)) else shap_out\n",
    "\n",
    "# 8) shap_data dict 생성 & 저장\n",
    "units     = latest['unit'].tolist()\n",
    "shap_data = {\n",
    "    u: {'feature_names': feats, 'values': shap_vals[i].tolist()}\n",
    "    for i, u in enumerate(units)\n",
    "}\n",
    "joblib.dump(shap_data, OUT_PKL)\n",
    "print(f\"✔ shap_values.pkl 생성 완료: {OUT_PKL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e7a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
